% !TEX root = research_paper.tex

\section{Simulation Study} % (fold)
\label{sec: sim_study}

We set up a simulation study to assess the performance of global parametric and local non-parametric methods along the bias and precision trade-off for the RDD treatment effect estimate. All estimators are challenged for three different data-generating processes (DGPs). The first DGP is based on a linear model, the second wave of datasets stem from a polynomial process of order four, and finally we relate the running variable and outcomes with help of sinus, co-sinus and polynomial functions. Randomness is introduced by an idiosyncratic element drawn from a mean-zero normal distribution with fixed variance. The data-generating processes are illustrated by FIGURE where a very small random noise component is specified and observations are collapsed into bins containing averages for reasons of illustration.

Our results are based on 250 randomly drawn datasets with 500 observations each per DGP, and in total we draw 750 datasets. Aside from the average estimated treatment effects, we compute the estimates' standard deviation, coverage probability and mean squared error. The coverage probability informs us about the share of estimate's 95-percent confidence bands which cover the true treatment effect. Repeating the simulation infinitely many times, the coverage probability of an unbiased estimator approaches 95 percent if the significance level is chosen to be five percent. While we can infer information on the bias the coverage probability does not inform us about the estimator's precision -- i.e. an imprecisely measured coefficient has larger confidence intervals which will likely (here, with an observed share of 95 percent if it is estimated without bias) cover the true coefficient. Parametric treatment effect estimation is conducted for models with polynomial degrees varying from zero\footnote{For a model of polynomial degree of zero the treatment effect estimate is a simple difference in means of outcomes left vis-Ã -vis right of the cutoff.} to five. For local linear regression we report results for different bandwidths based on leave-one-out cross-validation, the rule-of-thumb selection procedure and 50 percent under- as well as 200 percent oversmoothing thereof. Note that 50 and 200 percent of the rule-of-thumb bandwidth correspond to the grid's lower and upper bound where the cross-validation bandwidth is chosen from.

TABLE X shows results for parametrically estimating the treatment effect for data coming from the three DGPs. As expected the linear global model is well suited to recover the treatment effect of .75, and so are higher order polynomials -- this is foremost the case since higher order polynomial models comprise the (true) linear model as a special case. The respective coverage probabilities hover around the expected value of 95 and we would need to increase the Monte Carlo simulations to approach .95 precisely. In line with econometric theory the standard deviation and mean squared error as measures on precision increase in value with every extra polynomial specification [SOURCE?]. Simply comparing means unsurprisingly overestimates the treatment effect since we have a larger slope coefficient on the cutoff's right side. Panel (b) of TABLE X exhibits results for the \textit{polynomial} DGP and we see that starting with the cubic model higher order polynomials estimate the treatment effect consistently. As was noted before, increasing the number of regressors through additional polynomials makes the treatment effect estimate less precise indicated by larger standard deviations and mean squared errors. The linear and quadratic specification perform rather poorly exhibiting large bias and variance. The drawbacks of inflexible functional form assumptions and the restriction to regard all data globally available become apparent by glancing at results in Panel C. While the linear and cubic model are already off by quite a bit, other parametric specifications are not able to resemble their estimates around the true value. Without prior knowledge of the true data-generating process the researcher may collect visual signs of the observed data but can hardly be guided by the present results.

Similarly, we collect performance measures for estimates from local linear regression for four bandwidths in TABLE Y. When consulting the results, three remarks claim attention. First, restricting the data through the bandwidth pays off in terms of bias. The rule-of-thumb, its undersmoothed companion and the cross-validated bandwidth all lead to consistent results in terms of estimated average treatment effects, with its strongest game for the linear model -- perhaps not to the reader's surprise as we locally fit a linear model. Only the oversmoothed rule-of-thumb bandwidth struggles to assist the estimator to recover on average the true treatment effect, in the polynomial and non-parametric DGPs. Second, the coverage probabilities across models and DGPs lacks behind the targeted 95 percent. Third, a larger bandwidth increases precision and tends to introduce bias. To make this claim we compare the undersmoothed against the oversmoothed rule-of-thumb bandwidth and notice that both precision measures are smaller choosing a larger bandwidth. This comes at the cost of introducing bias in the estimate for the second and third DGP. For the first DGP the bias is not present since the underlying true model is linear throughout, and we notice that the estimates with the larger bandwidth are on average more precise. In fact, when comparing the results from the local linear and the global linear model (for the linear DGP) we find that the global linear model is most precise -- it features the maximum bandwidth by construction.

In all, we find in our simulation study that parametric methods are consistently estimating the treatment effect in scenarios where the true data-generating process is based on a polynomial relationship between the running variable and the outcome. They perform poorly for data with less structure, if no classic polynomials are involved or the regression model features too few polynomial degrees to cope with richer data structures. While non-parametric methods are capturing linear and polynomial relationships in data they really shine -- in comparison to global methods -- if the underlying data-generating processes become more involved. Choosing a smaller bandwidth improves consistency at the cost of precision. The key take-away from this investigation is: local methods face less risk of introducing bias to estimates and can handle more complex data structures. However, if the underlying data-generating process is well-known tailored global methods may be used to increase precision.



TODO: Table 5.3??









