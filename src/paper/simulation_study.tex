% !TEX root = research_paper.tex

We set up a simulation study to assess the performance of global parametric and local non-parametric methods along the bias and precision trade-off for the RDD treatment effect estimate. All estimators are challenged for three different data-generating processes (DGPs). The first DGP is based on a linear model, the second wave of datasets stem from a polynomial process of order four, and finally we relate the running variable and outcomes with help of sinus, co-sinus and polynomial functions. Randomness is introduced by an idiosyncratic element drawn from a mean-zero normal distribution with fixed variance. The data-generating processes are illustrated by Figure \ref{fig: dgp} where a very small random noise component is specified and observations are collapsed into bins containing averages for reasons of illustration.

Our results are based on 250 randomly drawn datasets (Monte Carlo repetitions) per DGP with 500 observations. Aside from the average estimated treatment effects, we compute the estimates' standard deviation over all Monte Carlo repetitions, coverage probability and mean squared error. Note that the mean squared error is computed with respect to the estimated and true treatment effect and is not to be confused with the mean squared error computed in the cross-validation procedure. The coverage probability informs us about the share of estimate's 95-percent confidence bands which cover the true treatment effect. Repeating the simulation infinitely many times, the coverage probability of an unbiased estimator approaches 95 percent if the significance level is chosen to be five percent. While we can infer information on the bias the coverage probability does not inform us about the estimator's precision -- i.e. an imprecisely measured coefficient has larger confidence intervals which will likely (here, with an observed share of 95 percent if it is estimated without bias) cover the true coefficient. Parametric treatment effect estimation is conducted for models with polynomial degrees varying from zero\footnote{For a model of polynomial degree of zero the treatment effect estimate is a simple difference in means of outcomes left vis-Ã -vis right of the cutoff.} to five. For local linear regression we report results for different bandwidths based on leave-one-out cross-validation, the rule-of-thumb selection procedure and 50 percent under- as well as 200 percent oversmoothing thereof. Note that 50 and 200 percent of the rule-of-thumb bandwidth correspond to the grid's lower and upper bound where the cross-validation bandwidth is chosen from.

Table \ref{tab: perf_para} shows results for parametrically estimating the treatment effect for data coming from the three DGPs. As expected the linear global model is well suited to recover the treatment effect of .75, and so are higher order polynomials -- this is foremost the case since higher order polynomial models comprise the (true) linear model as a special case. The respective coverage probabilities hover around the expected value of 95 and we would need to increase the Monte Carlo simulations to approach .95 precisely. In line with econometric theory the standard deviation and mean squared error as measures on precision increase in value with every extra polynomial specification [SOURCE?]. Simply comparing means unsurprisingly overestimates the treatment effect since we have a larger slope coefficient on the cutoff's right side. Panel (b) of Table \ref{tab: perf_para} exhibits results for the \textit{polynomial} DGP and we see that starting with the cubic model higher order polynomials estimate the treatment effect consistently. As was noted before, increasing the number of regressors through additional polynomials makes the treatment effect estimate less precise indicated by larger standard deviations and mean squared errors. The linear and quadratic specification perform rather poorly exhibiting large bias and variance. The drawbacks of inflexible functional form assumptions and the restriction to regard all data globally available become apparent by glancing at results in Panel C. While the linear and cubic model are already off by quite a bit, other parametric specifications are not able to resemble their estimates around the true value. Without prior knowledge of the true data-generating process the researcher may collect visual signs of the observed data but can hardly be guided by the present results.

Similarly, we collect performance measures for estimates from local linear regression for four bandwidths in Table \ref{tab: perf_nonpara}. When consulting the results, three remarks claim attention. First, restricting the data through the bandwidth pays off in terms of bias. The rule-of-thumb, its undersmoothed companion and the cross-validated bandwidth all lead to consistent results in terms of estimated average treatment effects, with its strongest game for the linear model -- perhaps not to the reader's surprise as we locally fit a linear model. Only the oversmoothed rule-of-thumb bandwidth struggles to assist the estimator to recover on average the true treatment effect, in the polynomial and non-parametric DGPs. Second, the coverage probabilities across models and DGPs lacks behind the targeted 95 percent. The fact that common test-statistics derived from non-parametric methods are oftentimes not correctly centered is well-known (cf. \cite{wasserman}).\footnote{When a mean squared error optimal bandwidth is chosen, most test statistics suffer from an asymptotic bias which is usually corrected by undersmoothing or by estimating the bias directly.} Third, a larger bandwidth increases precision and tends to introduce bias. To make this claim we compare the undersmoothed against the oversmoothed rule-of-thumb bandwidth and notice that both precision measures are smaller choosing a larger bandwidth. This comes at the cost of introducing bias in the estimate for the second and third DGP. For the first DGP the bias is not present since the underlying true model is linear throughout, and we notice that the estimates with the larger bandwidth are on average more precise. In fact, when comparing the results from the local linear and the global linear model (for the linear DGP) we find that the global linear model is most precise -- it features the maximum bandwidth by construction.

To compare the bandwidth selection procedures and the numeric values that they select in more detail, Table $\dots$ reports measures of descriptive statistics across the 250 Monte Carlo repetitions considered. We can see that depending on the underlying data generating process, the values that are selected by the rule-of-thumb and cross-validation procedure follow different patterns. For the linear DGP, the rule-of-thumb procedure tends to select far smaller bandwidths than cross-validation on average and also the standard deviation of all the values selected lies below the cross-validated ones. Cross-validation has a larger standard deviation in this case, also indicated by the minimum and maximum lying far apart which gives rise to the supposition that a significantly different bandwidth is selected in every run. For the polynomial DGP however, the means of the values selected are quite close to each other, the standard deviation is significantly reduced, in particular when using cross-validation which now lies below the one of the rule-of-thumb procedure. Finally, when considering the non-parametric DGP, the standard deviation of bandwidths selected by cross-validation shrinks further and also the mean of the values selected lies a bit below the ones of the rule-of-thumb procedure. A key takeaway of this analysis is that the more curved and non-linear the DGP, the smaller the average bandwidth selected by cross-validation which is an intuitive result as the features of the underlying regression function can be grasped better by fitting local models the further away the true function is from a linear one. For the rule-of-thumb bandwidth however, this pattern is less prevalent and the mean of the bandwidths selected rather stays at roughly the same level with only a minor decrease from model one to three.

In all, we find in our simulation study that parametric methods are consistently estimating the treatment effect in scenarios where the true data-generating process is based on a polynomial relationship between the running variable and the outcome. They perform poorly for data with less structure, if no classic polynomials are involved or the regression model features too few polynomial degrees to cope with richer data structures. While non-parametric methods are capturing linear and polynomial relationships in data they really shine -- in comparison to global methods -- if the underlying data-generating processes become more involved. Choosing a smaller bandwidth improves consistency at the cost of precision. The key take-away from this investigation is: local methods face less risk of introducing bias to estimates and can handle more complex data structures. However, if the underlying data-generating process is well-known tailored global methods may be used to increase precision.











